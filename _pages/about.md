---
permalink: /
title: "About me"
excerpt: "About me"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---
Hi, welcome to my homepage! I am a Senior Research Scientist on the AI Research team at Cruise. My research focuses on vision-language models, data mining, and 3D vision for autonomous driving. Previously, I earned my Ph.D. in Computer Science from The University of Texas at Austin, where I had the privilege of working with Prof. [Qiang Liu](https://www.cs.utexas.edu/~lqiang/index.html). I completed my bachelor’s degree in Mathematics at Zhejiang University in 2017 and my master’s degree in Statistics at Purdue University in 2019. I have also gained valuable industry experience through internships: as an AI Research Intern at Cruise in the summer of 2022, a Machine Learning Research Intern at Waymo in the summer of 2021, and a Machine Learning Research Intern at Meta in 2020.

Selected Research Projects
======

**Generative Diffusion Model**
------

Improved diffusion bridge for learning normal and special data distribution.

**Mao Ye**, Lemeng Wu and Qiang Liu. [First Hitting Diffusion Models for Generating Manifold, Graph and Categorical Data.](https://proceedings.neurips.cc/paper_files/paper/2022/file/ae87d80f5a0f3ee5c5643448f9599d1b-Paper-Conference.pdf) *NeurIPS 2022*

Learning Efficient Neural Network
------
Network pruning is a successful technique for learning a compact network model. I work on a strong theory-oriented pruning algorithm based on greedy optimization. We show that
* A small network learned by our algorithm is guaranteed to be better than a small network learned by direct training.
* Theoretically, it is important to fine-tune the pruned network instead of retraining it.

**Mao Ye**<sup> * </sup>, Lemeng Wu<sup> * </sup> and Qiang Liu. [Greedy Optimization Provably Wins the Lottery:
Logarithmic Number of Winning Tickets is Enough.](https://arxiv.org/pdf/2010.15969.pdf) *NeurIPS 2020*

**Mao Ye**, Chengyue Gong<sup> * </sup>, Lizhen Nie<sup> * </sup>, Denny Zhou, Adam Klivans and Qiang Liu. [Good Subnetworks Provably Exists: Pruning via Greedy Forward Selection.](https://proceedings.icml.cc/static/paper_files/icml/2020/1781-Paper.pdf) *ICML 2020*

**3D Detection**
-----
I studied 3D object detection for both label efficiency and model efficiency.

**Mao Ye**, Gregory P. Meyer, Yuning Chai, and Qiang Liu. [Efficient Transformer-based 3D Object Detection with Dynamic Token Halting.](https://openaccess.thecvf.com/content/ICCV2023/papers/Ye_Efficient_Transformer-based_3D_Object_Detection_with_Dynamic_Token_Halting_ICCV_2023_paper.pdf) *ICCV 2023*

**Mao Ye**, Chenxi Liu, Maoqing Yao, Weiyue Wang, Zhaoqi Leng, Charles R. Qi, and Dragomir Anguelov. [Multi-Class 3D Object Detection with Single-Class Supervision.](https://arxiv.org/pdf/2205.05703) *ICRA 2022*

Certified Robustness
-----
We propose a functional constraint optimization framework for cerfified adversarial defense using random smoothing. A general class of smoothing distribution is proposed for image classification task. And a special discrete smoothing distribution is proposed for text classification.

Dinghuai Zhang<sup> * </sup>, **Mao Ye**<sup> * </sup>, Chengyue Gong<sup> * </sup> and Qiang Liu. [Black-Box Certification with Randomized Smoothing: A Functional Optimization Based Framework.](https://arxiv.org/pdf/2002.09169.pdf) *Neurips 2020*

**Mao Ye**<sup> * </sup>, Chengyue Gong<sup> * </sup> and Qiang Liu. [SAFER: A Structure-free Approach for Certified Robustness to Adversarial Word Substitutions.](https://www.aclweb.org/anthology/2020.acl-main.317.pdf) *ACL 2020*

**Bi-level Optimization**
-----
A simple first-order gradient algorithm for Bilevel optimization without convexity assumption for both upper and lower problems.

**Mao Ye**<sup> * </sup>, Bo Liu<sup> * </sup>, Stephen Wright and Qiang Liu. BOME! Bilevel Optimization Made Easy: A Simple First-Order Approach. *NeurIPS 2022*

**Causal Inference**
-----
I develop network architecture and optimization techniques for treatment estimation using deep neural networks.

Lizhen Nie<sup> * </sup>, **Mao Ye**<sup> * </sup>, Qiang Liu and Dan Nicolae. [Varying Coefficient Neural Network with Functional Targeted Regularization for Estimating Continuous Treatment Effects.](https://openreview.net/pdf?id=RmB-88r9dL) *ICLR 2021* (**Oral Presentation**, accept rate 1.77%)

Uncertainty Estimation
-----
I develop simple surrogate loss to improve the particle quality in bootstrap.

**Mao Ye** and Qiang Liu. [Centroid Approximation for Bootstrap: Improving Particle Quality at Inference.](https://arxiv.org/pdf/2110.08720.pdf) *ICML 2022*

Feature Selection
-----
Selecting useful feature is important for ML systems. We develop a new drop-out-one loss that accurately detects useful features for difficult task with highly correlated features. Our approach is very easy to implement and is guaranteed to select all useful features and discard all useless features when there is enough data.

**Mao Ye**<sup> * </sup> and Yan Sun<sup> * </sup>. [Variable Selection via Penalized Neural Network: a Drop-Out-One Loss Approac.](http://proceedings.mlr.press/v80/ye18b/ye18b.pdf) *ICML 2018*

Sampling Method
------
Sampling method is important for Bayesian inference and RL. We develop a special samping dynamics that utilizes the collected samples to guide the sampler to explore the unexplored region and thus improve the sampling quality.

**Mao Ye**<sup> * </sup>, Tongzheng Ren<sup> * </sup> and Qiang Liu. [Stein Self-Repulsive Dynamics: Benefits from Past Samples.](https://arxiv.org/pdf/2002.09070.pdf) *NeurIPS 2020*

Services
======
Conference reviewer: ICML, NeurIPS, ICLR, CVPR

Journal reviewer: JMLR, TAMPI, Machine Learning, Neurocomputing, Canadian journal of statistics


